{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ENSEMBLE TECHNIQUES**"
      ],
      "metadata": {
        "id": "F02rw-zIFm0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ASSIGNMENT**"
      ],
      "metadata": {
        "id": "di6M987SFt8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRACTICAL QUESTIONS :**\n",
        "\n",
        "**QUESTION 21 - 45 :**\n",
        "\n"
      ],
      "metadata": {
        "id": "1TaBV8f-FyEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Train a Bagging Classifier using Decision Trees and print accuracy.\n",
        "\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.ensemble import BaggingClassifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    X, y = load_iris(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "    model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FFS129iNGFYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Train a Bagging Regressor and evaluate using MSE\n",
        "\n",
        "    from sklearn.datasets import fetch_california_housing\n",
        "    from sklearn.ensemble import BaggingRegressor\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X, y = fetch_california_housing(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "    model = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "TncKIHZvIIrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Random Forest Classifier on Breast Cancer dataset & feature importance\n",
        "\n",
        "    from sklearn.datasets import load_breast_cancer\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    print(\"Feature Importances:\", rf.feature_importances_)\n"
      ],
      "metadata": {
        "id": "noIuZwABJBwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**24. Train a Random Forest Regressor and compare its performance with a single Decision Tree2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Random Forest Regressor vs Decision Tree\n",
        "\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    dt = DecisionTreeRegressor()\n",
        "    rf = RandomForestRegressor(n_estimators=100)\n",
        "\n",
        "    dt.fit(X_train, y_train)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"DT MSE:\", mean_squared_error(y_test, dt.predict(X_test)))\n",
        "    print(\"RF MSE:\", mean_squared_error(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "OB5iXnK-JPMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.  Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Compute OOB Score for Random Forest\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        "    )\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(\"OOB Score:\", rf.oob_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "jyCeNf9IK9IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**26.  Train a Bagging Classifier using SVM as a base estimator and print accuracy2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging Classifier using SVM\n",
        "\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    model = BaggingClassifier(\n",
        "    base_estimator=SVC(),\n",
        "    n_estimators=10\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "j4JStBM8LbyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**27. Train a Random Forest Classifier with different numbers of trees and compare accuracy2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Random Forest with different number of trees\n",
        "\n",
        "    for n in [10, 50, 100]:\n",
        "    rf = RandomForestClassifier(n_estimators=n)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"Trees {n} Accuracy:\", rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "l4jZxp8kLqV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**28.  Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging Classifier using Logistic Regression & AUC\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    model = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(max_iter=1000),\n",
        "    n_estimators=10\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    print(\"AUC:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "P1kUHEdgL5P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**29.  Train a Random Forest Regressor and analyze feature importance scores2**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Random Forest Regressor feature importance\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=100)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(rf.feature_importances_)\n"
      ],
      "metadata": {
        "id": "U4d57YyxMJNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Compare Bagging vs Random Forest\n",
        "\n",
        "    bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50)\n",
        "    rf = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "    bag.fit(X_train, y_train)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Bagging Accuracy:\", bag.score(X_test, y_test))\n",
        "    print(\"Random Forest Accuracy:\", rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "-Dym9GGmMS27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Hyperparameter tuning using GridSearchCV\n",
        "\n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    params = {'n_estimators':[50,100], 'max_depth':[None,10]}\n",
        "    grid = GridSearchCV(RandomForestClassifier(), params)\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    print(grid.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FMsaCWdsMpWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**32. Train a Bagging Regressor with different numbers of base estimators and compare performance**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging Regressor with different estimators\n",
        "\n",
        "    for n in [10, 50, 100]:\n",
        "    model = BaggingRegressor(n_estimators=n)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(n, mean_squared_error(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "iS5lZKDYM6UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**33.  Train a Random Forest Classifier and analyze misclassified samples**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Analyze misclassified samples\n",
        "\n",
        "    y_pred = rf.predict(X_test)\n",
        "    misclassified = X_test[y_pred != y_test]\n",
        "    print(\"Misclassified samples:\", misclassified.shape[0])\n"
      ],
      "metadata": {
        "id": "6zipLO4dNE9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**34.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging vs Single Decision Tree\n",
        "\n",
        "    dt = DecisionTreeClassifier()\n",
        "    bag = BaggingClassifier(dt, n_estimators=50)\n",
        "\n",
        "    dt.fit(X_train, y_train)\n",
        "    bag.fit(X_train, y_train)\n",
        "\n",
        "    print(\"DT:\", dt.score(X_test, y_test))\n",
        "    print(\"Bagging:\", bag.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "uxEuvJ6lNP_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**35.  Train a Random Forest Classifier and visualize the confusion matrix**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    print(confusion_matrix(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "uvnPyFZKNdIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Stacking Classifier\n",
        "\n",
        "    from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "    estimators = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))\n",
        "    ]\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression()\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "    print(stack.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "YNWAiFpENmfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**37. Train a Random Forest Classifier and print the top 5 most important features**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Top 5 important features\n",
        "\n",
        "    import numpy as np\n",
        "    indices = np.argsort(rf.feature_importances_)[-5:]\n",
        "    print(indices)\n"
      ],
      "metadata": {
        "id": "abgvp4WQN2A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Precision, Recall, F1\n",
        "\n",
        "    from sklearn.metrics import classification_report\n",
        "    print(classification_report(y_test, rf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "Vp9_XWSROEhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Effect of max_depth\n",
        "\n",
        "    for d in [5,10,None]:\n",
        "    rf = RandomForestClassifier(max_depth=d)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(d, rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "km9PFNX3OQPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging Regressor with different base estimators\n",
        "\n",
        "    from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "    for est in [DecisionTreeRegressor(), KNeighborsRegressor()]:\n",
        "    model = BaggingRegressor(est)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(mean_squared_error(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "tpR1LrvzOaT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "ROC-AUC Score\n",
        "\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    y_prob = rf.predict_proba(X_test)[:,1]\n",
        "    print(roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "id": "0NqfoBNoOrq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**42. Train a Bagging Classifier and evaluate its performance using cross-validation.**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "    from sklearn.model_selection import cross_val_score\n",
        "    scores = cross_val_score(bag, X, y, cv=5)\n",
        "    print(scores.mean())\n"
      ],
      "metadata": {
        "id": "-m1xeJfsO1bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**43. Train a Random Forest Classifier and plot the Precision-Recall curve**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Precision-Recall Curve\n",
        "\n",
        "    from sklearn.metrics import precision_recall_curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n"
      ],
      "metadata": {
        "id": "K9kskagrPC0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**44.  Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Stacking (RF + Logistic Regression)\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "    estimators=[('rf', RandomForestClassifier())],\n",
        "    final_estimator=LogisticRegression()\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "    print(stack.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "iVf7Ew2OPdfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.**\n",
        "\n",
        "**ANSWER:**\n",
        "\n",
        "Bagging Regressor with different bootstrap levels\n",
        "\n",
        "    for b in [True, False]:\n",
        "    model = BaggingRegressor(bootstrap=b)\n",
        "    model.fit(X_train, y_train)\n",
        "    print(b, mean_squared_error(y_test, model.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "L0m0c91dPpDL"
      }
    }
  ]
}